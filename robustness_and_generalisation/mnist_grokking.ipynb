{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_points = 1000\n",
    "test_points= 1000\n",
    "robustness_points=500\n",
    "optimization_steps = 100000\n",
    "batch_size = 200\n",
    "weight_decay = 0.01\n",
    "lr = 1e-3\n",
    "initialization_scale = 8.0\n",
    "download_directory = \".\"\n",
    "\n",
    "depth = 3\n",
    "width = 200\n",
    "\n",
    "log_freq = math.ceil(optimization_steps / 150)\n",
    "\n",
    "device = 'cpu'\n",
    "dtype = torch.float64\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(dtype)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torchvision.datasets.MNIST(root=download_directory, train=True, \n",
    "    transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test = torchvision.datasets.MNIST(root=download_directory, train=False, \n",
    "    transform=torchvision.transforms.ToTensor(), download=True)\n",
    "\n",
    "sub_train = torch.utils.data.Subset(train, range(train_points))\n",
    "sub_test = torch.utils.data.Subset(test, range(test_points))\n",
    "train_loader = torch.utils.data.DataLoader(sub_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "def compute_accuracy(network, dataset, device, N=2000, batch_size=50):\n",
    "    with torch.no_grad():\n",
    "        N = min(len(dataset), N)\n",
    "        batch_size = min(batch_size, N)\n",
    "        dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x, labels in islice(dataset_loader, N // batch_size):\n",
    "            logits = network(x.to(device))\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "            correct += torch.sum(predicted_labels == labels.to(device))\n",
    "            total += x.size(0)\n",
    "        return (correct / total).item()\n",
    "\n",
    "def compute_loss(network, dataset, device, N=2000, batch_size=50):\n",
    "    with torch.no_grad():\n",
    "        N = min(len(dataset), N)\n",
    "        batch_size = min(batch_size, N)\n",
    "        dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        one_hots = torch.eye(10, 10).to(device)\n",
    "        total = 0\n",
    "        points = 0\n",
    "        for x, labels in islice(dataset_loader, N // batch_size):\n",
    "            y = network(x.to(device))\n",
    "            total += nn.MSELoss()(y, one_hots[labels]).item()\n",
    "            points += len(labels)\n",
    "        return total / points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Flatten()]\n",
    "for i in range(depth):\n",
    "    if i == 0:\n",
    "        layers.append(nn.Linear(784, width))\n",
    "        layers.append(nn.ReLU())\n",
    "    elif i == depth - 1:\n",
    "        layers.append(nn.Linear(width, 10))\n",
    "    else:\n",
    "        layers.append(nn.Linear(width, width))\n",
    "        layers.append(nn.ReLU())\n",
    "mlp = nn.Sequential(*layers).to(device)\n",
    "with torch.no_grad():\n",
    "    for p in mlp.parameters():\n",
    "        p.data = initialization_scale * p.data\n",
    "\n",
    "optimizer = torch.optim.AdamW(mlp.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "norms = []\n",
    "last_layer_norms = []\n",
    "log_steps = []\n",
    "\n",
    "memorised_saved=False\n",
    "generalised_saved=False\n",
    "\n",
    "steps = 0\n",
    "one_hots = torch.eye(10, 10).to(device)\n",
    "with tqdm(total=optimization_steps) as pbar:\n",
    "    for x, labels in islice(cycle(train_loader), optimization_steps):\n",
    "        if (steps < 30) or (steps < 150 and steps % 10 == 0) or steps % log_freq == 0:\n",
    "            train_losses.append(compute_loss(mlp, sub_train, device, N=len(train)))\n",
    "            train_accuracies.append(compute_accuracy(mlp, sub_train, device, N=len(train)))\n",
    "            test_losses.append(compute_loss(mlp, sub_test, device, N=len(test)))\n",
    "            test_accuracies.append(compute_accuracy(mlp, sub_test, device, N=len(test)))\n",
    "            log_steps.append(steps)\n",
    "            with torch.no_grad():\n",
    "                total = sum(torch.pow(p, 2).sum() for p in mlp.parameters())\n",
    "                norms.append(float(np.sqrt(total.item())))\n",
    "                last_layer = sum(torch.pow(p, 2).sum() for p in mlp[-1].parameters())\n",
    "                last_layer_norms.append(float(np.sqrt(last_layer.item())))\n",
    "            pbar.set_description(\"L: {0:1.1e}|{1:1.1e}. A: {2:2.1f}%|{3:2.1f}%\".format(\n",
    "                train_losses[-1],\n",
    "                test_losses[-1],\n",
    "                train_accuracies[-1] * 100, \n",
    "                test_accuracies[-1] * 100))\n",
    "            \n",
    "        if train_accuracies[-1]>0.99 and not(memorised_saved):\n",
    "            save_log_file=open('save_log.txt','w')\n",
    "            save_log_file.write(f'initialization_scale={initialization_scale}\\n')\n",
    "            save_log_file.write(f'memorised:step={steps},train_accuracy={train_accuracies[-1]},test_accuracy={test_accuracies[-1]}\\n')\n",
    "            save_log_file.close()\n",
    "            torch.save(mlp.state_dict(),'memorised_model.pth')\n",
    "            memorised_saved=True\n",
    "        \n",
    "        if test_accuracies[-1]>0.85 and not(generalised_saved):\n",
    "            save_log_file=open('save_log.txt','a')\n",
    "            save_log_file.write(f'generalised:step={steps},train_accuracy={train_accuracies[-1]},test_accuracy={test_accuracies[-1]}')\n",
    "            save_log_file.close()\n",
    "            torch.save(mlp.state_dict(),'generalised_model.pth')\n",
    "            generalised_saved=True\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y = mlp(x.to(device))\n",
    "        loss = nn.MSELoss()(y, one_hots[labels])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "        pbar.update(1)\n",
    "    if generalised_saved:\n",
    "        save_log_file=open('save_log.txt','a')\n",
    "        save_log_file.write(f'generalised:step={steps},train_accuracy={train_accuracies[-1]},test_accuracy={test_accuracies[-1]}')\n",
    "        save_log_file.close()\n",
    "        torch.save(mlp.state_dict(),'generalised_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1, 1, 1)\n",
    "plt.plot(log_steps, train_accuracies, color='red', label='train')\n",
    "plt.plot(log_steps, test_accuracies, color='green', label='test')\n",
    "plt.xscale('log')\n",
    "plt.xlim(10, None)\n",
    "plt.xlabel(\"Optimization Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=(0.015, 0.75))\n",
    "\n",
    "\n",
    "plt.legend(loc=(0.015, 0.65))\n",
    "plt.title(f\"depth-3 width-200 ReLU MLP on MNIST\\nUnconstrained Optimization Î± = {initialization_scale}\", fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'train_test_acc_init_{initialization_scale}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustness_score(model,perturbation_increment=0.01):\n",
    "    robustness_set=torch.utils.data.Subset(test,range(test_points,test_points+robustness_points))\n",
    "    robustness_loader=torch.utils.data.DataLoader(robustness_set,batch_size=1,shuffle=False)\n",
    "    perturbations=np.zeros(len(robustness_loader))\n",
    "\n",
    "    def verification(img,perturbation,checks=5):\n",
    "        verified=True\n",
    "        for n in range(5):\n",
    "            perturbation_img=2*torch.rand_like(img)-1\n",
    "            perturbation_img/=torch.norm(perturbation_img)\n",
    "            if model(img+perturbation*perturbation_img).argmax()!=label:\n",
    "                return False\n",
    "        return verified\n",
    "\n",
    "    pbar=tqdm(enumerate(robustness_loader),total=len(robustness_loader))\n",
    "    for k,(img,label) in pbar:\n",
    "        perturbation=0\n",
    "        while verification(img,perturbation):\n",
    "            pbar.set_description(f'perturbation={perturbation:.2f}')\n",
    "            perturbation+=perturbation_increment\n",
    "        perturbations[k]=perturbation\n",
    "    return perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Flatten()]\n",
    "for i in range(depth):\n",
    "    if i == 0:\n",
    "        layers.append(nn.Linear(784, width))\n",
    "        layers.append(nn.ReLU())\n",
    "    elif i == depth - 1:\n",
    "        layers.append(nn.Linear(width, 10))\n",
    "    else:\n",
    "        layers.append(nn.Linear(width, width))\n",
    "        layers.append(nn.ReLU())\n",
    "mlp_memorised = nn.Sequential(*layers).to(device)\n",
    "mlp_memorised.load_state_dict(torch.load('memorised_model.pth'))\n",
    "mlp_memorised.eval()\n",
    "robustness_memorised=robustness_score(mlp_memorised)\n",
    "mlp_generalised = nn.Sequential(*layers).to(device)\n",
    "mlp_generalised.load_state_dict(torch.load('generalised_model.pth'))\n",
    "mlp_generalised.eval()\n",
    "robustness_generalised=robustness_score(mlp_generalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(robustness_memorised[robustness_memorised!=0],alpha=0.5,label='memorised',bins=20,density=True)\n",
    "plt.hist(robustness_generalised[robustness_generalised!=0],alpha=0.5,label='generalised',bins=20,density=True)\n",
    "plt.title('\"Verified\" L2 Robustness')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(f'verification_histogram_{initialization_scale}.png')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
