{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from autoencoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAELoss(input,mu,logsigma,target,beta_kl):   \n",
    "    reconstruction_loss = nn.MSELoss()(target, input)\n",
    "    kl_div_loss = (0.5 * (mu ** 2 + torch.exp(2 * logsigma) - 1) - logsigma).mean() * beta_kl\n",
    "    return reconstruction_loss + kl_div_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(train: bool = True) -> Dataset:\n",
    "    img_size = 28\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    dataset = datasets.MNIST(\n",
    "        root = \"/data\",\n",
    "        train = train,\n",
    "        transform = transform,\n",
    "        download = True,\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_mnist = get_dataset()\n",
    "testset_mnist = get_dataset(train=False)\n",
    "\n",
    "sub_trainset_size=5000\n",
    "sub_trainset_mnist=Subset(trainset_mnist,np.random.choice(len(trainset_mnist),size=sub_trainset_size))\n",
    "single_loader=DataLoader(sub_trainset_mnist,batch_size=1)\n",
    "\n",
    "sub_trainset_mnist=torch.zeros((len(sub_trainset_mnist),28*28+1))\n",
    "for k,batch in enumerate(single_loader):\n",
    "    sub_trainset_mnist[k,:28*28]=batch[0].reshape(28*28)\n",
    "    sub_trainset_mnist[k,-1]=batch[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_noise_corruption_dataset(dataset,amplitude):\n",
    "    dataset_corrupted=torch.zeros((len(dataset),28*28+1))\n",
    "    for k in range(dataset_corrupted.shape[0]):\n",
    "        dataset_corrupted[k,:28*28]=dataset[k,:28*28]+amplitude*(2*torch.rand(28*28)-1)\n",
    "    return dataset_corrupted\n",
    "\n",
    "def trailing_corruption(dataset,length,p=0.5):\n",
    "    dataset_corrupted=torch.zeros((len(dataset),28*28+1))\n",
    "    for k in range(dataset_corrupted.shape[0]):\n",
    "        img=dataset[k,:28*28].reshape(28,28)\n",
    "        for row in range(28):\n",
    "            col=0\n",
    "            while col<28:\n",
    "                if col>=length:\n",
    "                    if (img[row,col-length:col]<0).sum()==length and (img[row,col]>0):\n",
    "                        if np.random.random()>p:\n",
    "                            for l in range(length+1):\n",
    "                                dataset_corrupted[k,28*row+col-l]=img[row,col]\n",
    "                        else:\n",
    "                            dataset_corrupted[k,28*row+col]=img[row,col]\n",
    "                    else:\n",
    "                        dataset_corrupted[k,28*row+col]=img[row,col]\n",
    "                else:\n",
    "                    dataset_corrupted[k,28*row+col]=img[row,col]\n",
    "                col+=1\n",
    "    return dataset_corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_mse(dataset,dataset_corrupted):\n",
    "    initial_mse=0\n",
    "    for n in range(dataset.shape[0]):\n",
    "        initial_mse+=torch.norm(dataset[k,:28*28]-dataset_corrupted[k,:28*28],p=2)\n",
    "    initial_mse=initial_mse/dataset.shape[0]\n",
    "    return initial_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_initial_samples(dataset,dataset_corrupted,num_sample_indices,directory,file_prefix):\n",
    "\n",
    "    sample_indicies=np.random.randint(low=0,high=dataset.shape[0],size=num_sample_indices)\n",
    "\n",
    "    mnist_sample=dataset[sample_indicies]\n",
    "    mnist_sample_corrupted=dataset_corrupted[sample_indicies]\n",
    "\n",
    "    fig,axs=plt.subplots(nrows=2,ncols=num_sample_indices,layout=\"constrained\")\n",
    "\n",
    "    for k in range(num_sample_indices):\n",
    "        axs[0][k].imshow(mnist_sample[k,:28*28].reshape(28,28),cmap='gray')\n",
    "        axs[0][k].axis('off')\n",
    "\n",
    "    for k in range(num_sample_indices):\n",
    "        axs[1][k].imshow(mnist_sample_corrupted[k,:28*28].reshape(28,28).detach().numpy(),cmap='gray')\n",
    "        axs[1][k].axis('off')\n",
    "    plt.savefig(f'{directory}/{file_prefix}_initial.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(dataset,model,opt,criterion,train_args,progress_bar_text,ground_truth=False):\n",
    "    dataset_size=dataset.shape[0]\n",
    "    vector_size=dataset.shape[1]-1\n",
    "    progress_bar=tqdm(range(train_args[\"epochs\"]))\n",
    "    for epoch in progress_bar:\n",
    "        perm=torch.randperm(dataset_size)\n",
    "        epoch_dataset=dataset[perm,:]\n",
    "        epoch_ground_truth=train_args[\"ground_truth_set\"][perm,:]\n",
    "        epoch_cycles=dataset_size//train_args[\"batch_size\"]\n",
    "        count=0\n",
    "        epoch_loss=0\n",
    "        for k in range(epoch_cycles+1):\n",
    "            if k==epoch_cycles:\n",
    "                n=dataset_size-k*train_args[\"batch_size\"]\n",
    "                img=epoch_dataset[-n:,:vector_size].reshape((n,1,28,28))\n",
    "                img_reconstructed = model(img)[0]\n",
    "                if ground_truth:\n",
    "                    loss = criterion(epoch_ground_truth[-n:,:vector_size].reshape((n,1,28,28)), img_reconstructed)\n",
    "                else:\n",
    "                    loss = criterion(img, img_reconstructed)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "                epoch_loss+=loss\n",
    "                count+=n\n",
    "            else:\n",
    "                img=epoch_dataset[k*train_args[\"batch_size\"]:(k+1)*train_args[\"batch_size\"],:vector_size].reshape((train_args[\"batch_size\"],1,28,28))\n",
    "                img_reconstructed = model(img)[0]\n",
    "                if ground_truth:\n",
    "                    loss = criterion(epoch_ground_truth[k*train_args[\"batch_size\"]:(k+1)*train_args[\"batch_size\"],:vector_size].reshape((train_args[\"batch_size\"],1,28,28)), img_reconstructed)\n",
    "                else:\n",
    "                    loss = criterion(img, img_reconstructed)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "                epoch_loss+=loss\n",
    "                count+=train_args[\"batch_size\"]\n",
    "        progress_bar.set_description(f\"{progress_bar_text} epoch={epoch+1}, loss={epoch_loss/count:.4f}\")\n",
    "    return epoch_loss/count\n",
    "\n",
    "def train_vae(dataset,model,opt,train_args,progress_bar_text,ground_truth=False):\n",
    "    dataset_size=dataset.shape[0]\n",
    "    vector_size=dataset.shape[1]-1\n",
    "    progress_bar=tqdm(range(train_args[\"epochs\"]))\n",
    "    for epoch in progress_bar:\n",
    "        perm=torch.randperm(dataset_size)\n",
    "        epoch_dataset=dataset[perm,:]\n",
    "        epoch_ground_truth=train_args[\"ground_truth_set\"][perm,:]\n",
    "        epoch_cycles=dataset_size//train_args[\"batch_size\"]\n",
    "        count=0\n",
    "        epoch_loss=0\n",
    "        for k in range(epoch_cycles+1):\n",
    "            if k==epoch_cycles:\n",
    "                n=dataset_size-k*train_args[\"batch_size\"]\n",
    "                img=epoch_dataset[-n:,:vector_size].reshape((n,1,28,28))\n",
    "                img_reconstructed,mu,logsigma,z = model(img)\n",
    "                if ground_truth:\n",
    "                    loss = VAELoss(epoch_ground_truth[-n:,:vector_size].reshape((n,1,28,28)),mu,logsigma, img_reconstructed,train_args[\"beta_kl\"])\n",
    "                else:\n",
    "                    loss = VAELoss(epoch_ground_truth[-n:,:vector_size].reshape((n,1,28,28)),mu,logsigma, img_reconstructed,train_args[\"beta_kl\"])\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "                epoch_loss+=loss\n",
    "                count+=n\n",
    "            else:\n",
    "                img=epoch_dataset[k*train_args[\"batch_size\"]:(k+1)*train_args[\"batch_size\"],:vector_size].reshape((train_args[\"batch_size\"],1,28,28))\n",
    "                img_reconstructed,mu,logsigma,z = model(img)\n",
    "                if ground_truth:\n",
    "                    loss = VAELoss(epoch_ground_truth[k*train_args[\"batch_size\"]:(k+1)*train_args[\"batch_size\"],:vector_size].reshape((train_args[\"batch_size\"],1,28,28)),mu,logsigma, img_reconstructed,train_args[\"beta_kl\"])\n",
    "                else:\n",
    "                    loss = VAELoss(epoch_ground_truth[k*train_args[\"batch_size\"]:(k+1)*train_args[\"batch_size\"],:vector_size].reshape((train_args[\"batch_size\"],1,28,28)),mu,logsigma, img_reconstructed,train_args[\"beta_kl\"])\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "                epoch_loss+=loss\n",
    "                count+=train_args[\"batch_size\"]\n",
    "        progress_bar.set_description(f\"{progress_bar_text} epoch={epoch+1}, loss={epoch_loss/count:.4f}\")\n",
    "    return epoch_loss/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataset(model,original_dataset,num_to_replace,num_samples,return_sample=True):\n",
    "    with torch.no_grad():\n",
    "        random_indices_sample=np.random.choice(num_to_replace,size=num_samples)\n",
    "        N=original_dataset.shape[0]\n",
    "        replaced_dataset=torch.zeros_like(original_dataset)\n",
    "        replaced_dataset[num_to_replace:,:]=original_dataset[num_to_replace:,:]\n",
    "        original_tensors_sample=torch.zeros((5,28*28))\n",
    "        updated_tensors_sample=torch.zeros((5,28*28))\n",
    "        count=0\n",
    "        for k in range(num_to_replace):\n",
    "            batch=original_dataset[k,:]\n",
    "            if k in random_indices_sample:\n",
    "                original_tensors_sample[count,:]=batch[:28*28]\n",
    "            replaced_dataset[k,:28*28]=model(batch[:28*28].reshape(1,1,28,28))[0].reshape(28*28)\n",
    "            replaced_dataset[k,-1]=batch[-1].item()\n",
    "            if k in random_indices_sample:\n",
    "                updated_tensors_sample[count,:]=replaced_dataset[N-num_to_replace+k,:28*28]\n",
    "                count+=1\n",
    "        if return_sample:\n",
    "            return replaced_dataset,original_tensors_sample,updated_tensors_sample\n",
    "        else:\n",
    "            return replaced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_autoencoder(dataset,ground_truth_set,num_interations,num_replacement,num_samples,progress_bar_text,model_args={\"latent_dim_size\":32,\"hidden_dim_size\":128},train_args={\"batch_size\":32,\"epochs\":10,\"lr\":1e-3,\"betas\":(0.5, 0.999)},retrain=True):\n",
    "    samples=[]\n",
    "    losses=[]\n",
    "    train_args[\"ground_truth_set\"]=ground_truth_set\n",
    "    for k in range(num_interations):\n",
    "        if k==0:\n",
    "            iter_progress_bar_text=f'Iteration {k}, '+progress_bar_text\n",
    "            model=Autoencoder(model_args[\"latent_dim_size\"],model_args[\"hidden_dim_size\"])\n",
    "            opt=torch.optim.Adam(model.parameters(),lr=train_args[\"lr\"],betas=train_args[\"betas\"])\n",
    "            criterion=nn.MSELoss()\n",
    "            losses.append(train_ae(dataset,model,opt,criterion,train_args,iter_progress_bar_text,ground_truth=True).item())\n",
    "            dataset,original_sample,_=update_dataset(model,dataset,num_replacement,num_samples)\n",
    "            samples.append(original_sample)\n",
    "        else:\n",
    "            iter_progress_bar_text=f'Iteration {k}, '+progress_bar_text\n",
    "            if retrain:\n",
    "                model=Autoencoder(model_args[\"latent_dim_size\"],model_args[\"hidden_dim_size\"])\n",
    "                opt=torch.optim.Adam(model.parameters(),lr=train_args[\"lr\"],betas=train_args[\"betas\"])\n",
    "                criterion=nn.MSELoss()\n",
    "            losses.append(train_ae(dataset,model,opt,criterion,train_args,iter_progress_bar_text,ground_truth=True).item())\n",
    "            dataset,_,ending_sample=update_dataset(model,dataset,num_replacement,num_samples)\n",
    "            samples.append(ending_sample)\n",
    "    return samples,losses\n",
    "\n",
    "def iterate_vautoencoder(dataset,ground_truth_set,num_interations,num_replacement,num_samples,progress_bar_text,model_args={\"latent_dim_size\":32,\"hidden_dim_size\":128},train_args={\"batch_size\":32,\"epochs\":10,\"lr\":1e-3,\"betas\":(0.5, 0.999),\"beta_kl\":0.1},retrain=True):\n",
    "    samples=[]\n",
    "    losses=[]\n",
    "    train_args[\"ground_truth_set\"]=ground_truth_set\n",
    "    for k in range(num_interations):\n",
    "        if k==0:\n",
    "            iter_progress_bar_text=f'Iteration {k}, '+progress_bar_text\n",
    "            model=VAE(model_args[\"latent_dim_size\"],model_args[\"hidden_dim_size\"])\n",
    "            opt=torch.optim.Adam(model.parameters(),lr=train_args[\"lr\"],betas=train_args[\"betas\"])\n",
    "            losses.append(train_vae(dataset,model,opt,train_args,iter_progress_bar_text,ground_truth=True).item())\n",
    "            dataset,original_sample,_=update_dataset(model,dataset,num_replacement,num_samples)\n",
    "            samples.append(original_sample)\n",
    "        else:\n",
    "            iter_progress_bar_text=f'Iteration {k}, '+progress_bar_text\n",
    "            if retrain:\n",
    "                model=VAE(model_args[\"latent_dim_size\"],model_args[\"hidden_dim_size\"])\n",
    "                opt=torch.optim.Adam(model.parameters(),lr=train_args[\"lr\"],betas=train_args[\"betas\"])\n",
    "            losses.append(train_vae(dataset,model,opt,train_args,iter_progress_bar_text,ground_truth=True).item())\n",
    "            dataset,_,ending_sample=update_dataset(model,dataset,num_replacement,num_samples)\n",
    "            samples.append(ending_sample)\n",
    "    return samples,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corruption_ae(corruption,amplitude,dataset,num_sample_indices,num_iteriations,num_replacement,iterations_to_plot,retrain):\n",
    "\n",
    "    if retrain:\n",
    "        retrain_text='retrain'\n",
    "    else:\n",
    "        retrain_text='no_retrain'\n",
    "    \n",
    "    amplitude_text='0'+str(amplitude).split('.')[-1]\n",
    "    if not(f'{retrain_text}_ae_{corruption}_corruption_{amplitude_text}' in os.listdir()):\n",
    "        os.mkdir(f'{retrain_text}_ae_{corruption}_corruption_{amplitude_text}')\n",
    "    directory_name=f'{retrain_text}_ae_{corruption}_corruption_{amplitude_text}'\n",
    "    file_prefix=f'{retrain_text}_{corruption}_corruption_{amplitude_text}'\n",
    "\n",
    "    if corruption=='random_noise':\n",
    "        dataset_corrupted=random_noise_corruption_dataset(dataset,amplitude)\n",
    "    elif corruption=='trailing':\n",
    "        dataset_corrupted=trailing_corruption(dataset,amplitude)\n",
    "    else:\n",
    "        raise ValueError('Specify valid corruption method')\n",
    "\n",
    "    plot_initial_samples(dataset,dataset_corrupted,num_sample_indices,directory_name,file_prefix)\n",
    "    init_mse=initial_mse(dataset,dataset_corrupted)\n",
    "    file=open(f'{directory_name}/{file_prefix}.txt','w')\n",
    "    file.write(f'Initial MSE = {init_mse}\\n')\n",
    "    progress_bar_text=f'Amplitude {amplitude} {corruption} corruption:'\n",
    "    samples,losses=iterate_autoencoder(dataset_corrupted,dataset,num_iteriations,num_replacement,num_sample_indices,progress_bar_text,retrain=retrain)\n",
    "    file.write(f'Losses = {losses}')\n",
    "    file.close()\n",
    "\n",
    "    fig,ax=plt.subplots(nrows=1,ncols=1)\n",
    "\n",
    "    fig.set_figheight(6)\n",
    "    fig.set_figwidth(6)\n",
    "\n",
    "\n",
    "    ax.plot(range(1,len(losses)+1),losses)\n",
    "    ax.set_xticks(np.arange(0,len(losses)+1,step=2))\n",
    "    plt.savefig(f'{directory_name}/{file_prefix}_losses.png')\n",
    "\n",
    "    fig,axs=plt.subplots(nrows=len(iterations_to_plot),ncols=num_sample_indices,layout=\"tight\")\n",
    "\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(5)\n",
    "\n",
    "    for row,iteration in enumerate(iterations_to_plot):\n",
    "        for col in range(num_sample_indices):\n",
    "            axs[row][col].imshow(samples[iteration][col,:].reshape(28,28),cmap='gray')\n",
    "            axs[row][col].axis('off')\n",
    "            if col==0:\n",
    "                axs[row][col].set_title(str(iteration))\n",
    "    plt.savefig(f'{directory_name}/{file_prefix}_training_samples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corruption_vae(corruption,amplitude,dataset,num_sample_indices,num_iteriations,num_replacement,iterations_to_plot,retrain):\n",
    "\n",
    "    if retrain:\n",
    "        retrain_text='retrain'\n",
    "    else:\n",
    "        retrain_text='no_retrain'\n",
    "\n",
    "    amplitude_text='0'+str(amplitude).split('.')[-1]\n",
    "    if not(f'{retrain_text}_vae_{corruption}_corruption_{amplitude_text}' in os.listdir()):\n",
    "        os.mkdir(f'{retrain_text}_vae_{corruption}_corruption_{amplitude_text}')\n",
    "    directory_name=f'{retrain_text}_vae_{corruption}_corruption_{amplitude_text}'\n",
    "    file_prefix=f'{retrain_text}_{corruption}_corruption_{amplitude_text}'\n",
    "\n",
    "    if corruption=='random_noise':\n",
    "        dataset_corrupted=random_noise_corruption_dataset(dataset,amplitude)\n",
    "    elif corruption=='trailing':\n",
    "        dataset_corrupted=trailing_corruption(dataset,amplitude)\n",
    "    else:\n",
    "        raise ValueError('Specify valid corruption method')\n",
    "\n",
    "    plot_initial_samples(dataset,dataset_corrupted,num_sample_indices,directory_name,file_prefix)\n",
    "    init_mse=initial_mse(dataset,dataset_corrupted)\n",
    "    file=open(f'{directory_name}/{file_prefix}.txt','w')\n",
    "    file.write(f'Initial MSE = {init_mse}\\n')\n",
    "    progress_bar_text=f'Amplitude {amplitude} {corruption} corruption:'\n",
    "    samples,losses=iterate_vautoencoder(dataset_corrupted,dataset,num_iteriations,num_replacement,num_sample_indices,progress_bar_text,retrain=retrain)\n",
    "    file.write(f'Losses = {losses}')\n",
    "    file.close()\n",
    "\n",
    "    fig,ax=plt.subplots(nrows=1,ncols=1)\n",
    "\n",
    "    fig.set_figheight(6)\n",
    "    fig.set_figwidth(6)\n",
    "\n",
    "\n",
    "    ax.plot(range(1,len(losses)+1),losses)\n",
    "    ax.set_xticks(np.arange(0,len(losses)+1,step=2))\n",
    "    plt.savefig(f'{directory_name}/{file_prefix}_losses.png')\n",
    "\n",
    "    fig,axs=plt.subplots(nrows=len(iterations_to_plot),ncols=num_sample_indices,layout=\"tight\")\n",
    "\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(5)\n",
    "\n",
    "    for row,iteration in enumerate(iterations_to_plot):\n",
    "        for col in range(num_sample_indices):\n",
    "            axs[row][col].imshow(samples[iteration][col,:].reshape(28,28),cmap='gray')\n",
    "            axs[row][col].axis('off')\n",
    "            if col==0:\n",
    "                axs[row][col].set_title(str(iteration))\n",
    "    plt.savefig(f'{directory_name}/{file_prefix}_training_samples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0, Amplitude 0 random_noise corruption: epoch=10, loss=0.0037: 100%|██████████| 10/10 [00:46<00:00,  4.66s/it]\n",
      "Iteration 1, Amplitude 0 random_noise corruption: epoch=10, loss=0.0034: 100%|██████████| 10/10 [00:48<00:00,  4.88s/it]\n",
      "Iteration 2, Amplitude 0 random_noise corruption: epoch=10, loss=0.0032: 100%|██████████| 10/10 [00:46<00:00,  4.68s/it]\n",
      "Iteration 3, Amplitude 0 random_noise corruption: epoch=10, loss=0.0031: 100%|██████████| 10/10 [00:44<00:00,  4.50s/it]\n",
      "Iteration 4, Amplitude 0 random_noise corruption: epoch=10, loss=0.0030: 100%|██████████| 10/10 [00:43<00:00,  4.36s/it]\n",
      "Iteration 5, Amplitude 0 random_noise corruption: epoch=10, loss=0.0029: 100%|██████████| 10/10 [00:42<00:00,  4.27s/it]\n",
      "Iteration 6, Amplitude 0 random_noise corruption: epoch=10, loss=0.0027: 100%|██████████| 10/10 [00:42<00:00,  4.26s/it]\n",
      "Iteration 7, Amplitude 0 random_noise corruption: epoch=10, loss=0.0026: 100%|██████████| 10/10 [00:48<00:00,  4.85s/it]\n",
      "Iteration 8, Amplitude 0 random_noise corruption: epoch=10, loss=0.0025: 100%|██████████| 10/10 [00:45<00:00,  4.51s/it]\n",
      "Iteration 9, Amplitude 0 random_noise corruption: epoch=10, loss=0.0024: 100%|██████████| 10/10 [00:41<00:00,  4.14s/it]\n",
      "Iteration 10, Amplitude 0 random_noise corruption: epoch=10, loss=0.0023: 100%|██████████| 10/10 [00:42<00:00,  4.27s/it]\n",
      "Iteration 11, Amplitude 0 random_noise corruption: epoch=10, loss=0.0022: 100%|██████████| 10/10 [00:45<00:00,  4.52s/it]\n",
      "Iteration 12, Amplitude 0 random_noise corruption: epoch=10, loss=0.0021: 100%|██████████| 10/10 [00:47<00:00,  4.77s/it]\n",
      "Iteration 13, Amplitude 0 random_noise corruption: epoch=10, loss=0.0021: 100%|██████████| 10/10 [00:51<00:00,  5.18s/it]\n",
      "Iteration 14, Amplitude 0 random_noise corruption: epoch=10, loss=0.0020: 100%|██████████| 10/10 [00:56<00:00,  5.68s/it]\n",
      "Iteration 15, Amplitude 0 random_noise corruption: epoch=10, loss=0.0019: 100%|██████████| 10/10 [02:06<00:00, 12.60s/it]\n",
      "Iteration 16, Amplitude 0 random_noise corruption: epoch=10, loss=0.0019: 100%|██████████| 10/10 [01:26<00:00,  8.69s/it]\n",
      "Iteration 17, Amplitude 0 random_noise corruption: epoch=10, loss=0.0018: 100%|██████████| 10/10 [00:51<00:00,  5.15s/it]\n",
      "Iteration 18, Amplitude 0 random_noise corruption: epoch=10, loss=0.0018: 100%|██████████| 10/10 [00:46<00:00,  4.61s/it]\n",
      "Iteration 19, Amplitude 0 random_noise corruption: epoch=10, loss=0.0017: 100%|██████████| 10/10 [00:42<00:00,  4.21s/it]\n",
      "Iteration 0, Amplitude 0 random_noise corruption: epoch=10, loss=0.0087: 100%|██████████| 10/10 [01:20<00:00,  8.00s/it]\n",
      "Iteration 1, Amplitude 0 random_noise corruption: epoch=10, loss=0.0087: 100%|██████████| 10/10 [01:10<00:00,  7.03s/it]\n",
      "Iteration 2, Amplitude 0 random_noise corruption: epoch=10, loss=0.0090: 100%|██████████| 10/10 [01:03<00:00,  6.34s/it]\n",
      "Iteration 3, Amplitude 0 random_noise corruption: epoch=10, loss=0.0091: 100%|██████████| 10/10 [00:41<00:00,  4.17s/it]\n",
      "Iteration 4, Amplitude 0 random_noise corruption: epoch=10, loss=0.0093: 100%|██████████| 10/10 [00:42<00:00,  4.25s/it]\n",
      "Iteration 5, Amplitude 0 random_noise corruption: epoch=10, loss=0.0092: 100%|██████████| 10/10 [00:42<00:00,  4.26s/it]\n",
      "Iteration 6, Amplitude 0 random_noise corruption: epoch=10, loss=0.0093: 100%|██████████| 10/10 [00:41<00:00,  4.18s/it]\n",
      "Iteration 7, Amplitude 0 random_noise corruption: epoch=10, loss=0.0093: 100%|██████████| 10/10 [00:43<00:00,  4.31s/it]\n",
      "Iteration 8, Amplitude 0 random_noise corruption: epoch=10, loss=0.0092: 100%|██████████| 10/10 [00:41<00:00,  4.17s/it]\n",
      "Iteration 9, Amplitude 0 random_noise corruption: epoch=10, loss=0.0092: 100%|██████████| 10/10 [00:41<00:00,  4.15s/it]\n",
      "Iteration 10, Amplitude 0 random_noise corruption: epoch=10, loss=0.0092: 100%|██████████| 10/10 [00:40<00:00,  4.01s/it]\n",
      "Iteration 11, Amplitude 0 random_noise corruption: epoch=10, loss=0.0091: 100%|██████████| 10/10 [00:42<00:00,  4.20s/it]\n",
      "Iteration 12, Amplitude 0 random_noise corruption: epoch=10, loss=0.0091: 100%|██████████| 10/10 [00:40<00:00,  4.10s/it]\n",
      "Iteration 13, Amplitude 0 random_noise corruption: epoch=10, loss=0.0089: 100%|██████████| 10/10 [00:40<00:00,  4.02s/it]\n",
      "Iteration 14, Amplitude 0 random_noise corruption: epoch=10, loss=0.0089: 100%|██████████| 10/10 [00:40<00:00,  4.03s/it]\n",
      "Iteration 15, Amplitude 0 random_noise corruption: epoch=10, loss=0.0089: 100%|██████████| 10/10 [00:38<00:00,  3.85s/it]\n",
      "Iteration 16, Amplitude 0 random_noise corruption: epoch=10, loss=0.0089: 100%|██████████| 10/10 [00:39<00:00,  3.99s/it]\n",
      "Iteration 17, Amplitude 0 random_noise corruption: epoch=10, loss=0.0088: 100%|██████████| 10/10 [00:40<00:00,  4.05s/it]\n",
      "Iteration 18, Amplitude 0 random_noise corruption: epoch=10, loss=0.0088: 100%|██████████| 10/10 [00:39<00:00,  3.93s/it]\n",
      "Iteration 19, Amplitude 0 random_noise corruption: epoch=10, loss=0.0088: 100%|██████████| 10/10 [00:40<00:00,  4.00s/it]\n",
      "Iteration 0, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0037: 100%|██████████| 10/10 [00:40<00:00,  4.09s/it]\n",
      "Iteration 1, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0033: 100%|██████████| 10/10 [00:38<00:00,  3.80s/it]\n",
      "Iteration 2, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0032: 100%|██████████| 10/10 [00:45<00:00,  4.59s/it]\n",
      "Iteration 3, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0031: 100%|██████████| 10/10 [00:40<00:00,  4.04s/it]\n",
      "Iteration 4, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0030: 100%|██████████| 10/10 [00:40<00:00,  4.05s/it]\n",
      "Iteration 5, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0029: 100%|██████████| 10/10 [00:38<00:00,  3.89s/it]\n",
      "Iteration 6, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0028: 100%|██████████| 10/10 [00:39<00:00,  3.93s/it]\n",
      "Iteration 7, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0027: 100%|██████████| 10/10 [00:39<00:00,  3.92s/it]\n",
      "Iteration 8, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0026: 100%|██████████| 10/10 [00:39<00:00,  3.92s/it]\n",
      "Iteration 9, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0025: 100%|██████████| 10/10 [00:39<00:00,  3.93s/it]\n",
      "Iteration 10, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0024: 100%|██████████| 10/10 [00:39<00:00,  3.97s/it]\n",
      "Iteration 11, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0023: 100%|██████████| 10/10 [00:39<00:00,  3.98s/it]\n",
      "Iteration 12, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0022: 100%|██████████| 10/10 [00:39<00:00,  3.92s/it]\n",
      "Iteration 13, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0022: 100%|██████████| 10/10 [00:38<00:00,  3.90s/it]\n",
      "Iteration 14, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0021: 100%|██████████| 10/10 [00:45<00:00,  4.54s/it]\n",
      "Iteration 15, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0020: 100%|██████████| 10/10 [00:39<00:00,  4.00s/it]\n",
      "Iteration 16, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0019: 100%|██████████| 10/10 [00:40<00:00,  4.01s/it]\n",
      "Iteration 17, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0019: 100%|██████████| 10/10 [00:37<00:00,  3.80s/it]\n",
      "Iteration 18, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0018: 100%|██████████| 10/10 [00:39<00:00,  4.00s/it]\n",
      "Iteration 19, Amplitude 0.3 random_noise corruption: epoch=10, loss=0.0018: 100%|██████████| 10/10 [00:40<00:00,  4.01s/it]\n",
      "Iteration 0, Amplitude 4 trailing corruption: epoch=10, loss=0.0090: 100%|██████████| 10/10 [00:42<00:00,  4.21s/it]\n",
      "Iteration 1, Amplitude 4 trailing corruption: epoch=10, loss=0.0091: 100%|██████████| 10/10 [00:40<00:00,  4.09s/it]\n",
      "Iteration 2, Amplitude 4 trailing corruption: epoch=10, loss=0.0093: 100%|██████████| 10/10 [00:43<00:00,  4.39s/it]\n",
      "Iteration 3, Amplitude 4 trailing corruption: epoch=10, loss=0.0094: 100%|██████████| 10/10 [00:42<00:00,  4.22s/it]\n",
      "Iteration 4, Amplitude 4 trailing corruption: epoch=10, loss=0.0096: 100%|██████████| 10/10 [00:40<00:00,  4.05s/it]\n",
      "Iteration 5, Amplitude 4 trailing corruption: epoch=10, loss=0.0096: 100%|██████████| 10/10 [00:41<00:00,  4.17s/it]\n",
      "Iteration 6, Amplitude 4 trailing corruption: epoch=10, loss=0.0095: 100%|██████████| 10/10 [00:40<00:00,  4.03s/it]\n",
      "Iteration 7, Amplitude 4 trailing corruption: epoch=10, loss=0.0094: 100%|██████████| 10/10 [00:40<00:00,  4.06s/it]\n",
      "Iteration 8, Amplitude 4 trailing corruption: epoch=10, loss=0.0094: 100%|██████████| 10/10 [00:44<00:00,  4.42s/it]\n",
      "Iteration 9, Amplitude 4 trailing corruption: epoch=10, loss=0.0093: 100%|██████████| 10/10 [00:40<00:00,  4.09s/it]\n",
      "Iteration 10, Amplitude 4 trailing corruption: epoch=10, loss=0.0093: 100%|██████████| 10/10 [00:40<00:00,  4.09s/it]\n",
      "Iteration 11, Amplitude 4 trailing corruption: epoch=10, loss=0.0092: 100%|██████████| 10/10 [00:40<00:00,  4.10s/it]\n",
      "Iteration 12, Amplitude 4 trailing corruption: epoch=10, loss=0.0091: 100%|██████████| 10/10 [00:40<00:00,  4.05s/it]\n",
      "Iteration 13, Amplitude 4 trailing corruption: epoch=10, loss=0.0090: 100%|██████████| 10/10 [00:40<00:00,  4.02s/it]\n",
      "Iteration 14, Amplitude 4 trailing corruption: epoch=10, loss=0.0090: 100%|██████████| 10/10 [00:41<00:00,  4.11s/it]\n",
      "Iteration 15, Amplitude 4 trailing corruption: epoch=10, loss=0.0089: 100%|██████████| 10/10 [00:41<00:00,  4.12s/it]\n",
      "Iteration 16, Amplitude 4 trailing corruption: epoch=10, loss=0.0088: 100%|██████████| 10/10 [00:39<00:00,  3.98s/it]\n",
      "Iteration 17, Amplitude 4 trailing corruption: epoch=10, loss=0.0088: 100%|██████████| 10/10 [00:40<00:00,  4.00s/it]\n",
      "Iteration 18, Amplitude 4 trailing corruption: epoch=10, loss=0.0088: 100%|██████████| 10/10 [00:40<00:00,  4.03s/it]\n",
      "Iteration 19, Amplitude 4 trailing corruption: epoch=10, loss=0.0088: 100%|██████████| 10/10 [00:39<00:00,  3.92s/it]\n"
     ]
    }
   ],
   "source": [
    "corruption_ae('random_noise',0,sub_trainset_mnist,5,20,5000,[0,9,19],retrain=False)\n",
    "corruption_vae('random_noise',0,sub_trainset_mnist,5,20,5000,[0,9,19],retrain=False)\n",
    "corruption_ae('random_noise',0.3,sub_trainset_mnist,5,20,5000,[0,9,19],retrain=False)\n",
    "corruption_vae('trailing',4,sub_trainset_mnist,5,20,5000,[0,9,19],retrain=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
