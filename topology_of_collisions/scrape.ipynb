{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code modified from https://github.com/tsb0601/MultiMon/blob/main/scrape.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "import torch\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv \n",
    "from pycocotools.coco import COCO\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rows = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions(annotations_path):\n",
    "    coco = COCO(annotations_path)\n",
    "\n",
    "    img_ids = coco.getImgIds()\n",
    "\n",
    "    all_captions = []\n",
    "    for img_id in img_ids:\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        img_captions = [ann['caption'].lower() for ann in anns]\n",
    "        all_captions.extend(img_captions)\n",
    "\n",
    "    return all_captions\n",
    "\n",
    "def write_unique_rows(row, writer):\n",
    "    \n",
    "    key1 = (row[2], row[3])\n",
    "    key2 = (row[0], row[1])\n",
    "    key3 = (row[1], row[0])\n",
    "\n",
    "    if (key1 not in unique_rows) and (key2 not in unique_rows) and (key3 not in unique_rows):\n",
    "        unique_rows.add(key1)\n",
    "        unique_rows.add(key2)\n",
    "        unique_rows.add(key3)\n",
    "        \n",
    "        writer.writerow(row)\n",
    "\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=''\n",
    "url=\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "file_path = os.path.join(data_dir, 'annotations.zip')\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Downloading the annotations...\")\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, 'annotations')):\n",
    "    print(\"Extracting the annotations...\")\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco():\n",
    "\n",
    "    annotations_train_path = os.path.join(data_dir, 'annotations', 'captions_train2017.json')\n",
    "    annotations_val_path = os.path.join(data_dir, 'annotations', 'captions_val2017.json')\n",
    "\n",
    "    all_captions_train = load_captions(annotations_train_path)\n",
    "    all_captions_val = load_captions(annotations_val_path)\n",
    "\n",
    "    all_captions = all_captions_train + all_captions_val\n",
    "\n",
    "    print(f\"Total number of captions (train): {len(all_captions_train)}\")\n",
    "    print(f\"Total number of captions (val): {len(all_captions_val)}\")\n",
    "    print(f\"Total number of captions (train + val): {len(all_captions)}\")\n",
    "\n",
    "    return all_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premises=load_coco()\n",
    "similarity_threshold=0.9\n",
    "batch_size=1024\n",
    "num_premises=20*batch_size\n",
    "corpus_data='COCO'\n",
    "\n",
    "# Compute the embeddings for each batch of premises\n",
    "bert_text_embeds_prompts = []\n",
    "for i in tqdm(range(0, num_premises, batch_size)): #len(premises)\n",
    "    premises_batch = premises[i:i+batch_size]\n",
    "    with torch.no_grad():\n",
    "        text_embeds_prompts_batch = bert_model.encode(premises_batch)\n",
    "\n",
    "    text_embeds_prompts_batch = torch.from_numpy(text_embeds_prompts_batch)\n",
    "    text_embeds_prompts_batch = F.normalize(text_embeds_prompts_batch, dim=1)\n",
    "\n",
    "    bert_text_embeds_prompts.append(text_embeds_prompts_batch)\n",
    "\n",
    "# Concatenate the embeddings for all batches\n",
    "bert_text_embeds_prompts = torch.cat(bert_text_embeds_prompts, dim=0)\n",
    "torch.save(bert_text_embeds_prompts,f'{data_dir}\\\\bert_text_embeds_prompts.pt')\n",
    "\n",
    "# split the premises into batches\n",
    "premises_batches = [premises[i:i+batch_size] for i in range(0, num_premises, batch_size)]\n",
    "\n",
    "# compute the embeddings for each batch of premises\n",
    "text_embeds_prompts = torch.zeros(num_premises, 768)\n",
    "for i, premises_batch in enumerate(tqdm(premises_batches)):\n",
    "    tok = tokenizer(premises_batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_outputs = clip_model.text_model(**tok)\n",
    "    text_embeds = text_outputs[1]\n",
    "    text_embeds = clip_model.text_projection(text_embeds)\n",
    "    text_embeds_prompt = F.normalize(text_embeds, dim=1)\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min(start_idx + batch_size, num_premises)\n",
    "    text_embeds_prompts[start_idx:end_idx, :] = text_embeds_prompt\n",
    "torch.save(text_embeds_prompts,f'{data_dir}\\\\clip_text_embeds_prompts.pt')\n",
    "\n",
    "similar_pairs = []\n",
    "\n",
    "for i in tqdm(range(0, num_premises, batch_size)):\n",
    "    batch_premises = premises[i:i+batch_size]\n",
    "    batch_text_embeds_prompts = text_embeds_prompts[i:i+batch_size]\n",
    "    bert_batch_text_embeds_prompts = bert_text_embeds_prompts[i:i+batch_size]\n",
    "    \n",
    "    similarity_matrix = torch.matmul(batch_text_embeds_prompts, text_embeds_prompts.t())\n",
    "    bert_similarity_matrix = torch.matmul(bert_batch_text_embeds_prompts, bert_text_embeds_prompts.t())\n",
    "    \n",
    "    mask = (similarity_matrix > similarity_threshold) & (abs(similarity_matrix - bert_similarity_matrix) > 0.2)\n",
    "\n",
    "    j_indices, k_indices = mask.nonzero(as_tuple=True)\n",
    "\n",
    "    for j, k in zip(j_indices.tolist(), k_indices.tolist()):\n",
    "        similarity_score = similarity_matrix[j, k].item()\n",
    "        bert_similarity_score = bert_similarity_matrix[j, k].item()\n",
    "        similar_pairs.append((batch_premises[j], premises[k],i+j,k,similarity_score,bert_similarity_score))\n",
    "\n",
    "file_path = f'similar_from_{corpus_data}2.csv'\n",
    "with open(file_path, mode='w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Sample 1','Sample 2','Sample 1 Index','Sample 2 Index','CLIP Similarity Score','BERT Similarity Score'])\n",
    "\n",
    "    negative_keywords = [\"there is no\", \"unable\", \"does not\", \"do not\", \"am not\", \"no image\", \"no picture\"]\n",
    "\n",
    "    similar_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    num_written = 0\n",
    "\n",
    "    for pair in tqdm(similar_pairs):\n",
    "\n",
    "            if not any(keyword in field for field in pair[:2] for keyword in negative_keywords):\n",
    "\n",
    "                prompt1, prompt2 = pair[0], pair[1]\n",
    "                \n",
    "                is_unique = write_unique_rows(pair, csv_writer)\n",
    "                if is_unique:\n",
    "                    num_written += 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
